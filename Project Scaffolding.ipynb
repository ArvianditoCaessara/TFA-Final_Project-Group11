{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627e8d",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project._\n",
    "\n",
    "* Code clarity: make sure the code conforms to:\n",
    "    * [ ] [PEP 8](https://peps.python.org/pep-0008/) - You might find [this resource](https://realpython.com/python-pep8/) helpful as well as [this](https://github.com/dnanhkhoa/nb_black) or [this](https://jupyterlab-code-formatter.readthedocs.io/en/latest/) tool\n",
    "    * [ ] [PEP 257](https://peps.python.org/pep-0257/)\n",
    "    * [ ] Break each task down into logical functions\n",
    "* The following files are submitted for the project (see the project's GDoc for more details):\n",
    "    * [ ] `README.md`\n",
    "    * [ ] `requirements.txt`\n",
    "    * [ ] `.gitignore`\n",
    "    * [ ] `schema.sql`\n",
    "    * [ ] 6 query files (using the `.sql` extension), appropriately named for the purpose of the query\n",
    "    * [x] Jupyter Notebook containing the project (this file!)\n",
    "* [x] You can edit this cell and add a `x` inside the `[ ]` like this task to denote a completed task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import math\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import pyarrow.parquet as pq\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import datetime\n",
    "from math import sin, cos, sqrt, atan2, radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b622a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any general notebook setup, like log formatting\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need, for example:\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "# add other constants to refer to any local data, e.g. uber & weather\n",
    "UBER_CSV = \"uberdata/uber_rides_sample.csv\"\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9118da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder 'taxidata' because of .gitignore\n",
    "try:\n",
    "    os.mkdir(\"taxidata\")\n",
    "    os.mkdir(\"sql_files\")\n",
    "\n",
    "except FileExistsError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf38168",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Define a function that calculates the distance between two coordinates in kilometers that **only uses the `math` module** from the standard library.\n",
    "* [ ] Taxi data:\n",
    "    * [ ] Use the `re` module, and the packages `requests`, BeautifulSoup (`bs4`), and (optionally) `pandas` to programmatically download the required CSV files & load into memory.\n",
    "    * You may need to do this one file at a time - download, clean, sample. You can cache the sampling by saving it as a CSV file (and thereby freeing up memory on your computer) before moving onto the next file. \n",
    "* [ ] Weather & Uber data:\n",
    "    * [ ] Download the data manually in the link provided in the project doc.\n",
    "* [ ] All data:\n",
    "    * [ ] Load the data using `pandas`\n",
    "    * [ ] Clean the data, including:\n",
    "        * Remove unnecessary columns\n",
    "        * Remove invalid data points (take a moment to consider what's invalid)\n",
    "        * Normalize column names\n",
    "        * (Taxi & Uber data) Remove trips that start and/or end outside the designated [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "    * [ ] (Taxi data) Sample the data so that you have roughly the same amount of data points over the given date range for both Taxi data and Uber data.\n",
    "* [ ] Weather data:\n",
    "    * [ ] Split into two `pandas` DataFrames: one for required hourly data, and one for the required daily daya.\n",
    "    * [ ] You may find that the weather data you need later on does not exist at the frequency needed (daily vs hourly). You may calculate/generate samples from one to populate the other. Just document what youâ€™re doing so we can follow along. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "\n",
    "The calculate_distance function calculates the approximate distance between pickup and drop off points using latitude and longitude coordinates. \n",
    "\n",
    "The add_distance_column function adds a column with the calculated distance values to the taxi data set to facilitate analysis going forward.   \n",
    "\n",
    "Note that these functions only use the 'math' module fron the standard library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(start_lat, start_lon, end_lat, end_lon):\n",
    "    \"\"\"\n",
    "    Calculates the approximate distance \n",
    "    between pickup and drop off points using \n",
    "    latitude and longitude coordinates.\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    start_lat:\n",
    "        Starting lattitude.\n",
    "    start_lon:\n",
    "        Starting longtitude.\n",
    "    end_lat:\n",
    "        Ending lattitude\n",
    "    end_lon:\n",
    "        Ending longtitude\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The calculated distance  \n",
    "    \"\"\"\n",
    "    \n",
    "    R = 6373.0\n",
    "\n",
    "    lat1 = radians(start_lat)\n",
    "    lon1 = radians(start_lon)\n",
    "    lat2 = radians(end_lat)\n",
    "    lon2 = radians(end_lon)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    \n",
    "    return round(distance, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(df):\n",
    "    \"\"\"\n",
    "    Adds distance column to a dataframe\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    df:\n",
    "        Data frame.\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    The data frame with distance column column added \n",
    "    \"\"\"\n",
    "    \n",
    "    distance = []\n",
    "    \n",
    "    for i in df.index:\n",
    "        estimated_distance = calculate_distance(df[\"pickup_latitude\"][i], df[\"pickup_longitude\"][i], df[\"dropoff_latitude\"][i], df[\"dropoff_longitude\"][i])\n",
    "        distance.append(estimated_distance)\n",
    "        \n",
    "    df[\"calculated_distance\"] = distance\n",
    "       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2132cbac",
   "metadata": {},
   "source": [
    "### Converting datetime\n",
    "\n",
    "Function transforms date columns from strings to datetime Python objects to facilitate analysis going forward.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e46f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_str_to_obj(date_time_str):\n",
    "    \"\"\"\n",
    "    Transforms date columns from strings to datetime Python objects\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    date_time_str:\n",
    "        Date time string.\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    Date time objects  \n",
    "    \"\"\"\n",
    "    \n",
    "    date_time_obj = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return date_time_obj    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "Regex is used to find relevant URLs for yellow taxi files within specified timeframe and programmatically download the files. Functions are then created to clean data sets. For example, unwanted columns are dropped, datapoints outside the specified NYC area are removed, and column names are normalized. This is done to facilitate more efficient and effective analysis in later steps. Details are highlighted in code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_csv_urls():\n",
    "    \"\"\"\n",
    "    Uses regex to find all taxi URLs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    All taxi links  \n",
    "    \"\"\"\n",
    "    TAXI_URL = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "    \n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    data = []\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    for link in soup(\"a\"):\n",
    "        name = link.get(\"title\")\n",
    "        date = link.get(\"href\")\n",
    "        date_pattern = r'201[012345]|2009'\n",
    "        if name == \"Yellow Taxi Trip Records\" and re.search(date_pattern, date):\n",
    "            if not re.search(r'2015\\-0[789]|2015\\-1[012]', date):\n",
    "                data.append(link.get(\"href\"))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89ad2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_lon_from_loc():\n",
    "    find_lat_lon = gpd.read_file(\"taxi_zones/taxi_zones.shp\")\n",
    "  \n",
    "    find_lat_lon = find_lat_lon.to_crs(4326)\n",
    "    lon = find_lat_lon.centroid.x \n",
    "    lat = find_lat_lon.centroid.y\n",
    "    find_lat_lon[\"lon\"] = lon\n",
    "    find_lat_lon[\"lat\"] = lat\n",
    "\n",
    "    return find_lat_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "459739ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lat_lon(df):\n",
    "    lon_border = [NEW_YORK_BOX_COORDS[0][1], NEW_YORK_BOX_COORDS[1][1]]\n",
    "    lat_border = [NEW_YORK_BOX_COORDS[0][0], NEW_YORK_BOX_COORDS[1][0]]\n",
    "    \n",
    "    deleted_row = []\n",
    "    \n",
    "    for i in df.index:\n",
    "        if df[\"pickup_longitude\"][i] < lon_border[0] or df[\"pickup_longitude\"][i] > lon_border[1] or df[\"dropoff_longitude\"][i] < lon_border[0] or df[\"dropoff_longitude\"][i] > lon_border[1]:\n",
    "            deleted_row.append(i)\n",
    "            \n",
    "        elif df[\"pickup_latitude\"][i] < lat_border[0] or df[\"pickup_latitude\"][i] > lat_border[1] or df[\"dropoff_latitude\"][i] < lat_border[0] or df[\"dropoff_latitude\"][i] > lat_border[1]:\n",
    "            deleted_row.append(i)\n",
    "            \n",
    "            \n",
    "    df = df.drop(labels = deleted_row, axis=0)\n",
    "    \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdcad8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Normalize column for 2009 data\n",
    "    if \"Passenger_Count\" in df.columns :\n",
    "        df.rename(columns={\"Passenger_Count\": 'passenger_count', \"Fare_Amt\": 'fare_amount', \"Trip_Distance\": 'trip_distance'}, inplace=True)\n",
    "    \n",
    "    deleted_row = []\n",
    "    \n",
    "    for i in df.index:        \n",
    "        # Trips with zero passenger count\n",
    "        if df[\"passenger_count\"][i] < 1 or df[\"passenger_count\"][i] == False:\n",
    "            deleted_row.append(i)\n",
    "            \n",
    "        # Trips with no fare\n",
    "        elif df[\"fare_amount\"][i] <= 0 or df[\"fare_amount\"][i] == False:\n",
    "            deleted_row.append(i)\n",
    "            \n",
    "        # Trips with no distance between dropoff and pickup\n",
    "        elif df[\"trip_distance\"][i] <= 0 or df[\"trip_distance\"][i] == False:\n",
    "            deleted_row.append(i)\n",
    "            \n",
    "    df = df.drop(labels = deleted_row, axis=0)\n",
    "    \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url):\n",
    "    \n",
    "    # Check if we already have the data in local, otherwise download it\n",
    "    file_name = url[-31:]\n",
    "    \n",
    "    for i in os.listdir('./taxidata'):\n",
    "        if i == file_name:\n",
    "            df = pd.read_parquet(f\"taxidata/{file_name}\", engine='pyarrow')\n",
    "            return df\n",
    "    \n",
    "    \n",
    "    # Download and preprocessing data\n",
    "    find_lat_lon = get_lat_lon_from_loc() #get the lat-lon form .shp file\n",
    "    \n",
    "    # Read the data from url\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(f\"taxidata/{url[-31:]}\", \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    df = pd.read_parquet(f\"taxidata/{url[-31:]}\", engine='pyarrow') #Reading the data\n",
    "    df = df.sample(n=2564).reset_index(drop=True) #Take 2564 random sample\n",
    "\n",
    "    try:\n",
    "        #Clean the data\n",
    "        df = clean_data(df)\n",
    "    \n",
    "        # Rename the columns\n",
    "        if 'Start_Lon' in df.columns : #2009\n",
    "            df = df[[\"Trip_Pickup_DateTime\", \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\", \"Tip_Amt\"]]\n",
    "            df.rename(columns={\"Trip_Pickup_DateTime\": 'date_time', \"Start_Lon\": 'pickup_longitude', \"Start_Lat\": 'pickup_latitude', \"End_Lon\": 'dropoff_longitude', \"End_Lat\": 'dropoff_latitude', \"Tip_Amt\": 'tip_amount'}, inplace=True)\n",
    "        \n",
    "        elif 'pickup_longitude' in df.columns : #2010\n",
    "            df = df[[\"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"tip_amount\"]]\n",
    "            df.rename(columns={\"pickup_datetime\": 'date_time'}, inplace=True)\n",
    "\n",
    "        else:     \n",
    "            if \"tpep_pickup_datetime\" in df.columns : #2011-2014\n",
    "                df = df[[\"tpep_pickup_datetime\", \"PULocationID\", \"DOLocationID\", \"tip_amount\"]]\n",
    "                df.rename(columns={\"tpep_pickup_datetime\": 'date_time'}, inplace=True)\n",
    "        \n",
    "            else: df = df[[\"date_time\", \"trip_distance\", \"PULocationID\", \"DOLocationID\"]] #2015\n",
    "            \n",
    "            \n",
    "            # Finding the lat-lon using shp file\n",
    "            start_lon = []\n",
    "            start_lat = []\n",
    "            end_lon = []\n",
    "            end_lat = []\n",
    "        \n",
    "        \n",
    "            for i in range(len(df[\"PULocationID\"])):\n",
    "                start_point = df[\"PULocationID\"][i]\n",
    "                end_point = df[\"DOLocationID\"][i]\n",
    "            \n",
    "                if df[\"PULocationID\"][i] < 264 and df[\"DOLocationID\"][i] < 264: #Filter for NYC Area only\n",
    "                    index_location = find_lat_lon[find_lat_lon[\"LocationID\"] == start_point].index.values[0] \n",
    "                    start_lon.append(float(find_lat_lon[\"lon\"][index_location]))\n",
    "                    start_lat.append(float(find_lat_lon[\"lat\"][index_location]))\n",
    "                \n",
    "                    index_location = find_lat_lon[find_lat_lon[\"LocationID\"] == end_point].index.values[0] \n",
    "                    end_lon.append(float(find_lat_lon[\"lon\"][index_location]))\n",
    "                    end_lat.append(float(find_lat_lon[\"lat\"][index_location]))\n",
    "                \n",
    "                else: # Area outside NYC, to be deleted later\n",
    "                    start_lon.append(0)\n",
    "                    start_lat.append(0)\n",
    "                    end_lon.append(0)\n",
    "                    end_lat.append(0)\n",
    "                    \n",
    "        \n",
    "                     \n",
    "            df[\"pickup_longitude\"] = start_lon\n",
    "            df[\"pickup_latitude\"] = start_lat\n",
    "            df[\"dropoff_longitude\"] = end_lon\n",
    "            df[\"dropoff_latitude\"] = end_lat\n",
    "        \n",
    "            df = df.drop([\"PULocationID\", \"DOLocationID\"], axis=1)\n",
    "\n",
    "        \n",
    "        # Filter the lat-lon between (40.560445, -74.242330) and (40.908524, -73.717047)\n",
    "        df = filter_lat_lon(df)\n",
    "    \n",
    "        #Convert datetime str to python object\n",
    "        if isinstance(df[\"date_time\"][0], str):\n",
    "            for i in df.index:\n",
    "                df[\"date_time\"][i] = datetime_str_to_obj(df[\"date_time\"][i])\n",
    "    \n",
    "        # Calculate distance and add calculated_distance column\n",
    "        df = add_distance_column(df)\n",
    "        df= df[df['calculated_distance'] != 0]\n",
    "        \n",
    "        df = df[[\"date_time\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"tip_amount\", \"calculated_distance\"]]\n",
    "        \n",
    "    except IndexError:\n",
    "        os.remove(f\"taxidata/{url[-31:]}\")\n",
    "        get_and_clean_month_taxi_data(url)\n",
    " \n",
    "    \n",
    "    # Re-save the file\n",
    "    df.to_parquet(f\"taxidata/{url[-31:]}\")\n",
    "    \n",
    "    return df            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_csv_urls = find_taxi_csv_urls()\n",
    "    for csv_url in all_csv_urls:\n",
    "        # Get and clean the data from local or url\n",
    "        dataframe = get_and_clean_month_taxi_data(csv_url)\n",
    "        \n",
    "        # Put all the dataframe into a list\n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # Create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes, ignore_index=True)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "Functions are created to clean Uber data sets by removing trips with no passenger count and trips with no fare, as well as dropping unwanted columns and normalizing column names. This is done to facilitate more efficient and effective analysis in later steps. Details are highlighted in code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f8425bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_uber_data(df):\n",
    "    \n",
    "    deleted_row = []\n",
    "    \n",
    "    for i in df.index:\n",
    "        # Convert datetime str to python object\n",
    "        df['pickup_datetime'][i] = df['pickup_datetime'][i].replace(' UTC', '')\n",
    "        df['pickup_datetime'][i] = df['pickup_datetime'][i].replace('T', ' ')\n",
    "        df['pickup_datetime'][i] = datetime_str_to_obj(df['pickup_datetime'][i])\n",
    "\n",
    "        # Trips with zero passenger count\n",
    "        if df[\"passenger_count\"][i] < 1 or df[\"passenger_count\"][i] == False:\n",
    "            deleted_row.append(i)\n",
    "            \n",
    "        # Trips with no fare\n",
    "        elif df[\"fare_amount\"][i] <= 0 or df[\"fare_amount\"][i] == False:\n",
    "            deleted_row.append(i)\n",
    "            \n",
    "    df = df.drop(labels = deleted_row, axis=0)\n",
    "    df.rename(columns={\"pickup_datetime\": 'date_time'}, inplace=True)\n",
    "\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = clean_uber_data(df)\n",
    "    df = filter_lat_lon(df)\n",
    "    # Drop unecesarry column\n",
    "    df.drop(df.columns[[0, 1, 2, 8]], axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    uber_dataframe= uber_dataframe[uber_dataframe['calculated_distance'] != 0]\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "Functions are created to clean weather data by dropping unwanted columns and normalizing column names. Note that we have split the whole weather data into two separate sets for hourly and daily data respectively. In addition, we have specially curated the precipitation column to account for missing daily precipitation data by using the sum of hourly precipitation to estimate daily precipitation This is done to facilitate more efficient and effective analysis in later steps. Details are highlighted in code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    df2 = pd.read_csv('weatherdata/' + csv_file)\n",
    "    df2 = df2[['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed']]\n",
    "    \n",
    "    for i in df2.index:\n",
    "        # Convert datetime str to python object\n",
    "        df2['DATE'][i] = df2['DATE'][i].replace('T', ' ')\n",
    "        df2['DATE'][i] = datetime_str_to_obj(df2['DATE'][i])\n",
    "    \n",
    "        try:\n",
    "            df2[\"HourlyPrecipitation\"][i] = float(df2[\"HourlyPrecipitation\"][i])       \n",
    "            if(df2[\"HourlyPrecipitation\"][i] <= 0 or df2[\"HourlyPrecipitation\"][i] == False):\n",
    "                df2[\"HourlyPrecipitation\"][i] = None\n",
    "\n",
    "        except ValueError:\n",
    "            df2[\"HourlyPrecipitation\"][i] = None\n",
    "            \n",
    "            \n",
    "        try:\n",
    "            df2[\"HourlyWindSpeed\"][i] = int(df2[\"HourlyWindSpeed\"][i])\n",
    "            if (df2[\"HourlyWindSpeed\"][i] <= 0 or df2[\"HourlyWindSpeed\"][i] == False):\n",
    "                df2[\"HourlyWindSpeed\"][i] = None\n",
    "            \n",
    "        except ValueError:\n",
    "            df2[\"HourlyWindSpeed\"][i] = None\n",
    "       \n",
    "    \n",
    "    return df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a547fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    df3 = pd.read_csv('weatherdata/' + csv_file)\n",
    "\n",
    "    df3 = df3[['DATE', 'DailyAverageWindSpeed', 'DailyPrecipitation', 'Sunrise', 'Sunset']]\n",
    "    df3 = df3.dropna(thresh=3, how='all')\n",
    "    \n",
    "    \n",
    "    for i in df3.index:\n",
    "        # Convert datetime str to python object\n",
    "        df3['DATE'][i] = df3['DATE'][i].replace('T', ' ')\n",
    "        df3['DATE'][i] = datetime_str_to_obj(df3['DATE'][i])\n",
    "        \n",
    "        # Convert the sunrise sunset data to int\n",
    "        df3[\"Sunrise\"][i] = int(df3[\"Sunrise\"][i])\n",
    "        df3[\"Sunset\"][i] = int(df3[\"Sunset\"][i])\n",
    "        \n",
    "        try:\n",
    "            df3[\"DailyPrecipitation\"][i] = float(df3[\"DailyPrecipitation\"][i])\n",
    "            if(df3[\"DailyPrecipitation\"][i] <= 0 or df3[\"DailyPrecipitation\"][i] == False):\n",
    "                df3[\"DailyPrecipitation\"][i] = None\n",
    "        except ValueError:\n",
    "            df3[\"DailyPrecipitation\"][i] = None\n",
    "            \n",
    "        try:\n",
    "            df3[\"DailyAverageWindSpeed\"][i] = float(df3[\"DailyAverageWindSpeed\"][i])\n",
    "            if(df3[\"DailyAverageWindSpeed\"][i] <= 0 or df3[\"DailyAverageWindSpeed\"][i] == False):\n",
    "                df3[\"DailyAverageWindSpeed\"][i] = None\n",
    "        except ValueError:\n",
    "            df3[\"DailyAverageWindSpeed\"][i] = None\n",
    "    \n",
    "    \n",
    "    return df3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curated_daily_data(hourly_weather_data, daily_weather_data):\n",
    "    # Get the date from the hourly data\n",
    "    hourly_weather_data['h_temp'] = hourly_weather_data.apply(lambda row: row[\"DATE\"].strftime(\"%Y-%m-%d\") , axis=1)\n",
    "    \n",
    "    # Estimate the daily precipitation and windspeed from hourly data\n",
    "    hourly_weather_data['prep_temp'] = hourly_weather_data.apply(lambda row: hourly_weather_data.loc[hourly_weather_data['h_temp'] == row['h_temp'], 'HourlyPrecipitation'].sum() , axis=1)\n",
    "    hourly_weather_data['wind_temp'] = hourly_weather_data.apply(lambda row: hourly_weather_data.loc[hourly_weather_data['h_temp'] == row['h_temp'], 'HourlyWindSpeed'].mean() , axis=1)\n",
    "\n",
    "    # Drop the duplicates date and put it into temporary dataframe\n",
    "    df_temp = hourly_weather_data[[\"h_temp\", \"prep_temp\", \"wind_temp\"]].drop_duplicates(subset = [\"h_temp\"]).reset_index(drop=True)\n",
    "\n",
    "    # Get only the date from the daily data\n",
    "    daily_weather_data['d_temp'] = daily_weather_data.apply(lambda row: row[\"DATE\"].strftime(\"%Y-%m-%d\") , axis=1)\n",
    "    \n",
    "    # Check if the precipitation and windspeed from daily data is zero or null, it will use the estimated value from hourly data \n",
    "    for i in daily_weather_data.index:\n",
    "        if daily_weather_data[\"DailyAverageWindSpeed\"][i] and daily_weather_data[\"DailyAverageWindSpeed\"][i] > 0:\n",
    "            index = df_temp.index[df_temp['h_temp'] == daily_weather_data[\"d_temp\"][i]]\n",
    "        \n",
    "            df_temp[\"wind_temp\"][i] = daily_weather_data[\"DailyAverageWindSpeed\"][i]\n",
    "    \n",
    "        if daily_weather_data[\"DailyPrecipitation\"][i] and daily_weather_data[\"DailyPrecipitation\"][i] > 0:\n",
    "            index = df_temp.index[df_temp['h_temp'] == daily_weather_data[\"d_temp\"][i]]\n",
    "        \n",
    "            df_temp[\"prep_temp\"][i] = daily_weather_data[\"DailyPrecipitation\"][i]\n",
    "                \n",
    "    \n",
    "    return df_temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    # add some way to find all weather CSV files or just add the name/paths manually\n",
    "    weather_csv_files = [\"2009_weather.csv\", \"2010_weather.csv\", \"2011_weather.csv\", \"2012_weather.csv\", \"2013_weather.csv\", \"2014_weather.csv\", \"2015_weather.csv\"]\n",
    "\n",
    "\n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes, ignore_index=True)\n",
    "    daily_data = pd.concat(daily_dataframes, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    daily_sunrisesunset_data = daily_data[[\"DATE\", \"Sunrise\", \"Sunset\"]]\n",
    "    \n",
    "    daily_sunrisesunset_data['DATE'] = daily_sunrisesunset_data.apply(lambda row: row[\"DATE\"].strftime(\"%Y-%m-%d\") , axis=1)\n",
    "    \n",
    "    for i in daily_sunrisesunset_data.index:\n",
    "        x = str(daily_sunrisesunset_data[\"Sunrise\"][i])\n",
    "        daily_sunrisesunset_data[\"Sunrise\"][i] = f\"{x[:-4]}:{x[-4:-2]}\"\n",
    "        daily_sunrisesunset_data[\"Sunrise\"][i] = datetime.datetime.strptime(daily_sunrisesunset_data[\"Sunrise\"][i], '%H:%M').time()\n",
    "        y = str(daily_sunrisesunset_data[\"Sunset\"][i])\n",
    "        daily_sunrisesunset_data[\"Sunset\"][i] = f\"{y[:-4]}:{y[-4:-2]}\"\n",
    "        daily_sunrisesunset_data[\"Sunset\"][i] = datetime.datetime.strptime(daily_sunrisesunset_data[\"Sunset\"][i], '%H:%M').time()\n",
    "    \n",
    "    # Curated the daily data vs hourly data\n",
    "    new_daily_data = curated_daily_data(hourly_data, daily_data)  \n",
    "    \n",
    "    # Normalize the column name\n",
    "    hourly_data = hourly_data[[\"DATE\", \"HourlyPrecipitation\", \"HourlyWindSpeed\"]]\n",
    "    new_daily_data.rename(columns={\"h_temp\": 'DATE', \"prep_temp\": 'DailyPrecipitation', \"wind_temp\": 'DailyAverageWindSpeed' }, inplace=True)\n",
    "    \n",
    "    \n",
    "    return hourly_data, new_daily_data, daily_sunrisesunset_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "All the required functions written above are executed to obtain all the cleaned data sets used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n",
      "str nee\n"
     ]
    }
   ],
   "source": [
    "taxi_data = get_and_clean_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c36d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter taxi data\n",
    "taxi_data = taxi_data[[\"date_time\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"calculated_distance\", \"tip_amount\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ceb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data, daily_sunrisesunset_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE DELETED, READ DATA FROM EXCEL\n",
    "\n",
    "#a = pd.read_excel('taxi_data.xlsx')\n",
    "#b = pd.read_excel('uber_data.xlsx')\n",
    "#c = pd.read_excel('hourly_weather_data.xlsx')\n",
    "#d = pd.read_excel('daily_weather_data.xlsx')\n",
    "#e = pd.read_excel('daily_sunrisesunset_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "Using Python's `sqlite3` module, we create a database simply by creating a connection to it and then add information to our database. This is done by creating tables and specifying the schema for the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "connection = sqlite3.connect(\"final_project.db\")\n",
    "connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATETIME,\n",
    "    LATITUDE FLOAT,\n",
    "    LONGITUDE FLOAT,\n",
    "    HourlyPrecipitation FLOAT,\n",
    "    HourlyWindSpeed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    LATITUDE FLOAT,\n",
    "    LONGITUDE FLOAT,\n",
    "    DailyPrecipitation FLOAT,\n",
    "    DailyAverageWindSpeed FLOAT,\n",
    "    Sunrise FLOAT,\n",
    "    Sunset FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date_time DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    calculated_distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date_time DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    calculated_distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_SUNRISE_SUNSET_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_sunrise_sunset\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    Sunrise TIMESTAMP,\n",
    "    Sunset TIMESTAMP,\n",
    "    FOREIGN KEY(DATE) REFERENCES daily_weather(DATE)\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open('sql_files/schema.sql', \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)\n",
    "    f.write(DAILY_SUNRISE_SUNSET_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with connection:\n",
    "    connection.execute(HOURLY_WEATHER_SCHEMA)\n",
    "with connection:\n",
    "    connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "with connection:\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "with connection:\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)\n",
    "with connection:\n",
    "    connection.execute(DAILY_SUNRISE_SUNSET_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "Data is added to a SQLite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137365e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for key, value in table_to_df_dict.items():\n",
    "        print(key)\n",
    "        value.to_sql(name=key, con=connection, if_exists='append', index=False)\n",
    "    \n",
    "    return 'Success adding data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "    \"daily_sunrise_sunset\": daily_sunrisesunset_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(query)\n",
    "    \n",
    "    return f'Succes generate {outfile}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "\n",
    "Constructs and executes a query to categorize the number of yellow taxi trips for each hour of the day, in descending order. This is to determine which hour of the day was the most popular to take a yellow taxi, from 01-2009 through 06-2015. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77020452",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "SELECT \n",
    "    DISTINCT strftime('%H', date_time) AS time,\n",
    "    COUNT (*) as trip\n",
    "FROM taxi_trips\n",
    "WHERE date_time between '2009-01-01' AND '2015-06-30'\n",
    "GROUP BY time\n",
    "ORDER BY trip DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection:\n",
    "    result = connection.execute(QUERY_1)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed9cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, \"sql_files/taxi_trips_numbers_by_hour_in_descending_order.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a35ae",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "\n",
    "Constructs and executes a query to categorize the number of uber trips for each day of the week, in descending order. This is to determine what day of the week was the most popular to take an uber.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c8902",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"\n",
    "SELECT  case cast (strftime('%w', date_time) as integer)\n",
    "  when 0 then 'Sunday'\n",
    "  when 1 then 'Monday'\n",
    "  when 2 then 'Tuesday'\n",
    "  when 3 then 'Wednesday'\n",
    "  when 4 then 'Thursday'\n",
    "  when 5 then 'Friday'\n",
    "  else 'Saturday' end as day,\n",
    "  COUNT(*) as no_of_trip\n",
    "FROM uber_trips\n",
    "WHERE date_time between '2009-01-01' AND '2015-06-30'\n",
    "GROUP BY day\n",
    "ORDER BY no_of_trip DESC\n",
    "\"\"\"\n",
    "\n",
    "# WHERE tpep_pickup_datetime between '2009-01-01' AND '2015-06-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85904b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection:\n",
    "    result = connection.execute(QUERY_2)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d989dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, \"sql_files/uber_trips_numbers_for_each_day_in_descending_order.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2978f12",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "\n",
    "Constructs and executes a query to find the 95% percentile of distance travelled for all hired trips during July 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3 = \"\"\"\n",
    "WITH \n",
    "base \n",
    "AS (\n",
    "    SELECT\n",
    "        date,\n",
    "        calculated_distance,\n",
    "        ROW_NUMBER() OVER(ORDER BY calculated_distance ASC) AS row_num\n",
    "    FROM (\n",
    "        SELECT date(date_time) as date, calculated_distance\n",
    "        FROM taxi_trips\n",
    "        WHERE date between '2013-07-01' AND '2013-07-31'\n",
    "        UNION ALL\n",
    "        SELECT date(date_time) as date, calculated_distance\n",
    "        FROM uber_trips\n",
    "        WHERE date between '2013-07-01' AND '2013-07-31'\n",
    "    )\n",
    "    WHERE date between '2013-07-01' AND '2013-07-31'\n",
    "    ),\n",
    "    \n",
    "quantile\n",
    "AS (\n",
    "    SELECT\n",
    "        round(0.95 * COUNT(calculated_distance)) AS n_quantile\n",
    "    FROM\n",
    "        base\n",
    "    )\n",
    "    \n",
    "select \n",
    "base.calculated_distance \n",
    "from base\n",
    "join quantile\n",
    "on base.row_num = quantile.n_quantile\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219138f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection:\n",
    "    result = connection.execute(QUERY_3)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7c43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, \"sql_files/95%_percentile_of_distance_travelled_for_all_hired_trips_July_2013.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559ca1f",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "\n",
    "Constructs and executes a query to categorize the top 10 days with the greatest number of hired rides, and the corresponding average distance for each day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4 = \"\"\"\n",
    "SELECT\n",
    "    date,\n",
    "    COUNT (*) as no_of_trip,\n",
    "    AVG (calculated_distance)\n",
    "FROM (\n",
    "    SELECT date(date_time) AS date, calculated_distance\n",
    "    FROM taxi_trips\n",
    "    UNION ALL\n",
    "    SELECT date(date_time) AS date, calculated_distance\n",
    "    FROM uber_trips\n",
    ")\n",
    "WHERE date between '2009-01-01' AND '2009-12-31'\n",
    "GROUP BY date\n",
    "ORDER BY no_of_trip DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81daf27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOBEDELETED Read Data\n",
    "with connection:\n",
    "    result = connection.execute(QUERY_4)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b57a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, \"sql_files/top_10_days_with_greatest_number_hired_rides_and_corresponding_average_distance.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff445cb",
   "metadata": {},
   "source": [
    "### Query 5\n",
    "\n",
    "Constructs and executes a query to determine the top 10 windiest days in 2014 on average, and the number of hired trips made on those days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5 = \"\"\"\n",
    "WITH\n",
    "weather AS (\n",
    "SELECT\n",
    "    date(DATE) as date,\n",
    "    DailyAverageWindSpeed\n",
    "FROM\n",
    "    daily_weather\n",
    "WHERE date between '2014-01-01' AND '2014-12-31'\n",
    "ORDER BY DailyAverageWindSpeed DESC\n",
    "LIMIT 10),\n",
    "\n",
    "trip AS (\n",
    "SELECT\n",
    "    date,\n",
    "    COUNT (*) as no_of_trip\n",
    "FROM (\n",
    "    SELECT date(date_time) AS date\n",
    "    FROM taxi_trips\n",
    "    WHERE date between '2014-01-01' AND '2014-12-31'\n",
    "    UNION ALL\n",
    "    SELECT date(date_time) AS date\n",
    "    FROM uber_trips\n",
    "    WHERE date between '2014-01-01' AND '2014-12-31'\n",
    ")\n",
    "GROUP BY date\n",
    ")\n",
    "\n",
    "\n",
    "SELECT\n",
    "    weather.*,\n",
    "    trip.no_of_trip\n",
    "FROM\n",
    "    weather\n",
    "LEFT JOIN trip\n",
    "ON weather.date = trip.date\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13876397",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection:\n",
    "    result = connection.execute(QUERY_5)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, \"sql_files/top_10_windiest_days_2014_and_corresponding_number_of_hired_trips.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2884f762",
   "metadata": {},
   "source": [
    "### Query 6\n",
    "\n",
    "Constructs and executes a query to determine during hurricane Sandy in NYC (Oct 29-30, 2012), plus the week leading up and the week after, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive, and what was the sustained wind speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628bacbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6 = \"\"\"\n",
    "WITH\n",
    "weather AS (\n",
    "SELECT\n",
    "    strftime('%Y-%m-%d', DATE) AS date,\n",
    "    strftime('%H', DATE) AS time,\n",
    "    sum(HourlyPrecipitation) as Precipitation,\n",
    "    avg(HourlyWindSpeed) as Wind_Speed\n",
    "FROM\n",
    "    hourly_weather\n",
    "WHERE date between '2012-10-22' AND '2012-11-5'\n",
    "GROUP BY time\n",
    "),\n",
    "\n",
    "trip AS (\n",
    "SELECT\n",
    "    date,\n",
    "    time,\n",
    "    COUNT (*) as no_of_trip\n",
    "FROM(\n",
    "    SELECT \n",
    "        date(date_time) AS date, \n",
    "        strftime('%H', date_time) AS time\n",
    "    FROM taxi_trips\n",
    "    WHERE date between '2012-10-22' AND '2012-11-5'\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        date(date_time) AS date, \n",
    "        strftime('%H', date_time) AS time\n",
    "    FROM uber_trips\n",
    "    WHERE date between '2012-10-22' AND '2012-11-5'\n",
    ")\n",
    "GROUP BY date, time\n",
    ")\n",
    "\n",
    "\n",
    "SELECT\n",
    "    weather.date,\n",
    "    weather.time,\n",
    "    trip.no_of_trip,\n",
    "    weather.Precipitation,\n",
    "    weather.Wind_Speed\n",
    "FROM\n",
    "    weather\n",
    "LEFT JOIN trip\n",
    "ON weather.time = trip.time AND weather.date = trip.date\n",
    "ORDER BY weather.date\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection:\n",
    "    result = connection.execute(QUERY_6)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, \"sql_files/Hurricane_Sandy_tripsNumber_precipitation_windspeed_byHour.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cba3696",
   "metadata": {},
   "source": [
    "### Query 7 (Extra Credit)\n",
    "\n",
    "Constructs and executes a query to determine what are the top 10 sunrise times with highest average wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e6daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_EXTRA = \"\"\"\n",
    "SELECT\n",
    "    strftime('%H:%M', x.Sunrise) AS time,\n",
    "    avg(y.DailyAverageWindSpeed) AS average_windspeed\n",
    "FROM\n",
    "    daily_sunrise_sunset AS x\n",
    "JOIN daily_weather AS Y\n",
    "ON x.id = Y.id\n",
    "GROUP BY time\n",
    "ORDER BY average_windspeed DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f66984",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with connection:\n",
    "    result = connection.execute(QUERY_EXTRA)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c990ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_EXTRA, \"sql_files/top_10_sunrise_times_with_highest_average_windspeed.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1\n",
    "\n",
    "A bar chart is created to depict the total number of yellow taxi trips for each hour, from for 01-2009 through 06-2015. Data value labels are added for greater visualization. We chose to use bar charts for visualization to show segments of information and make comparisons most effectively. From the bar chart, 1900 hours was the most popular to take a yellow taxi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addlabels(x,y):\n",
    "        for i in range(len(x)):\n",
    "            plt.text(i,y[i],y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_1(data):\n",
    "    # preparing the dataset\n",
    "    hours = list(data.keys())\n",
    "    trips = list(data.values())\n",
    "\n",
    "    fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "    # creating the bar plot\n",
    "    bars = plt.bar(hours, trips, color ='lightblue', width = 0.4)\n",
    " \n",
    "    plt.xlabel(\"Hour\", fontsize = 13)\n",
    "    plt.ylabel(\"No. of trip\", fontsize = 13)\n",
    "    plt.title(\"No of yellow taxi trip in Hour basis\", fontsize = 13)\n",
    "    \n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "\n",
    " \n",
    "    \n",
    "            \n",
    "    addlabels(hours, trips)\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    highest = max(trips)\n",
    "    index = trips.index(highest)\n",
    "    \n",
    "    print(f\"the hour that was the most popular to take yellow taxi is {hours[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    QUERY = \"\"\"\n",
    "    SELECT \n",
    "        DISTINCT strftime('%H', date_time) AS time,\n",
    "        COUNT (*) as trip\n",
    "    FROM taxi_trips\n",
    "    WHERE date_time between '2009-01-01' AND '2015-06-30'\n",
    "    GROUP BY time\n",
    "    ORDER BY time ASC\n",
    "    \"\"\"\n",
    "    \n",
    "    with connection:\n",
    "        output = connection.execute(QUERY)\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for row in output:\n",
    "        data[str(row[0])] = row[1]\n",
    "        \n",
    "     \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1611d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qurated_taxidata = get_data_for_visual_1()\n",
    "plot_visual_1(qurated_taxidata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23e784",
   "metadata": {},
   "source": [
    "## Visualization 2\n",
    "\n",
    "A scatter plot is created to shows the average distance travelled per month for both taxis and Ubers combined. In addition, a 90% confidence interval, highlighted in blue, is created around the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from math import sqrt\n",
    "\n",
    "def plot_visual_2(data):\n",
    "    \n",
    "    month = list(data.keys())\n",
    "    avg_distance = list(data.values())\n",
    "    z = 1.645\n",
    "    \n",
    "    stdev = statistics.stdev(avg_distance)\n",
    "    confidence_interval = z * stdev / sqrt(len(avg_distance))\n",
    "    \n",
    "    figure = plt.figure(figsize = (10, 5))\n",
    "    \n",
    "    top = [i + confidence_interval for i in avg_distance]\n",
    "    bottom = [i - confidence_interval for i in avg_distance]\n",
    "\n",
    "    \n",
    " \n",
    "    plt.plot(month, avg_distance,'o', color='red')\n",
    "    plt.fill_between(month, bottom, top, color = 'lightblue', alpha = 0.5)\n",
    "    \n",
    "    \n",
    "    addlabels(month, avg_distance)\n",
    "    \n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Average Distance\")\n",
    "    plt.title(\"\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_2():\n",
    "    \n",
    "    QUERY = \"\"\"   \n",
    "    SELECT\n",
    "        month,\n",
    "        round(AVG (calculated_distance), 2)\n",
    "    FROM (\n",
    "        SELECT strftime('%m', date_time) AS month, calculated_distance\n",
    "        FROM taxi_trips\n",
    "        UNION ALL\n",
    "        SELECT strftime('%m', date_time) AS month, calculated_distance\n",
    "        FROM uber_trips\n",
    "    )   \n",
    "    GROUP BY month\n",
    "    \"\"\"\n",
    "    \n",
    "    with connection:\n",
    "        output = connection.execute(QUERY)\n",
    "    \n",
    "    data = {} \n",
    "    \n",
    "    for row in output:\n",
    "        data[str(row[0])] = row[1]\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization2_dataframe = get_data_for_visual_2()\n",
    "plot_visual_2(visualization2_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0569a",
   "metadata": {},
   "source": [
    "## Visualization 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545ec98",
   "metadata": {},
   "source": [
    "A bar chart is created to compare which day of the week was most popular for drop offs for the three major New York airports: LGA, JFK, and EWR. The website bboxfinder was used to obtain the coordinate boxes for the three airports. A Bar chart was chosen to show segments of information and make comparisons most effectively.\n",
    "\n",
    "The boxes for all airports was obtained from bboxfinder are:\n",
    "\n",
    "LGA = ((40.785860, -73.902975), (40.759019, -73.856576))\n",
    "JFK = ((40.676904, -73.824550), (40.623357, -73.741224))\n",
    "EWR = ((40.712517, -74.192865), (40.660366, -74.149802))\n",
    "\n",
    "The above coordinate will bse used in the SQL Query for this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d48877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_3(day, airport, trip):\n",
    "    \n",
    "    labels = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "    LGA = []\n",
    "    JFK = []\n",
    "    EWR = []\n",
    "    \n",
    "    for i in range(len(day)):\n",
    "        for j in labels:\n",
    "            if day[i] == j:\n",
    "                if airport[i] == 'LGA':\n",
    "                    LGA.append(trip[i])\n",
    "                elif airport[i] == 'JFK':\n",
    "                    JFK.append(trip[i])\n",
    "                elif airport[i] == 'EWR':\n",
    "                    EWR.append(trip[i]) \n",
    "    \n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.15  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    rects1 = ax.bar(x - width, LGA, width, label='LGA')\n",
    "    rects2 = ax.bar(x + width, JFK, width, label='JFK')\n",
    "    rects3 = ax.bar(x, EWR, width, label='EWR')\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('No of Trip', fontsize = 12)\n",
    "    ax.set_title('No of Trip based on Day')\n",
    "    ax.set_xticks(x, labels, fontsize = 12)\n",
    "    ax.legend(loc='upper right', fontsize = 12)\n",
    "    plt.ylim(0, 2500)\n",
    "    plt.rc('ytick', labelsize=10) \n",
    "\n",
    "    ax.bar_label(rects1, padding=3, fontsize = 12)\n",
    "    ax.bar_label(rects2, padding=3, fontsize = 12)\n",
    "    ax.bar_label(rects3, padding=3, fontsize = 12)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f52d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_3():\n",
    "    # Query SQL database for the data needed.\n",
    "    QUERY = \"\"\"\n",
    "    SELECT\n",
    "    day_of_week,\n",
    "    Airport,\n",
    "    SUM(number_of_trip) AS total_trip\n",
    "    FROM (\n",
    "    SELECT\n",
    "        CASE CAST(STRFTIME('%w', date_time) AS integer)\n",
    "        WHEN 0 THEN 'Sunday'\n",
    "        WHEN 1 THEN 'Monday'\n",
    "        WHEN 2 THEN 'Tuesday'\n",
    "        WHEN 3 THEN 'Wednesday'\n",
    "        WHEN 4 THEN 'Thursday'\n",
    "        WHEN 5 THEN 'Friday'\n",
    "        WHEN 6 THEN 'Saturday'\n",
    "        END AS day_of_week,\n",
    "        CASE \n",
    "            WHEN (dropoff_longitude BETWEEN -73.902975 AND -73.856576) \n",
    "            AND (dropoff_latitude BETWEEN 40.759019 AND 40.785860)\n",
    "            THEN 'LGA'\n",
    "            WHEN (dropoff_longitude BETWEEN -73.824550 AND -73.741224)\n",
    "            AND (dropoff_latitude BETWEEN 40.623357 AND 40.676904)\n",
    "            THEN 'JFK'\n",
    "            WHEN (dropoff_longitude BETWEEN -74.192865 AND -74.149802)\n",
    "            AND (dropoff_latitude BETWEEN 40.660366 AND 40.712517)\n",
    "            THEN 'EWR'\n",
    "        END AS 'Airport',\n",
    "        COUNT(*) AS number_of_trip        \n",
    "        \n",
    "    FROM taxi_trips\n",
    "    GROUP BY 1,2\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT\n",
    "        CASE CAST(STRFTIME('%w', date_time) AS integer)\n",
    "        WHEN 0 THEN 'Sunday'\n",
    "        WHEN 1 THEN 'Monday'\n",
    "        WHEN 2 THEN 'Tuesday'\n",
    "        WHEN 3 THEN 'Wednesday'\n",
    "        WHEN 4 THEN 'Thursday'\n",
    "        WHEN 5 THEN 'Friday'\n",
    "        WHEN 6 THEN 'Saturday'\n",
    "        END AS day_of_week,\n",
    "        CASE \n",
    "            WHEN (dropoff_longitude BETWEEN -73.902975 AND -73.856576) \n",
    "            AND (dropoff_latitude BETWEEN 40.759019 AND 40.785860)\n",
    "            THEN 'LGA'\n",
    "            WHEN (dropoff_longitude BETWEEN -73.824550 AND -73.741224)\n",
    "            AND (dropoff_latitude BETWEEN 40.623357 AND 40.676904)\n",
    "            THEN 'JFK'\n",
    "            WHEN (dropoff_longitude BETWEEN -74.192865 AND -74.149802)\n",
    "            AND (dropoff_latitude BETWEEN 40.660366 AND 40.712517)\n",
    "            THEN 'EWR'\n",
    "        END AS 'Airport',\n",
    "        COUNT(*) AS number_of_trip        \n",
    "        \n",
    "    FROM uber_trips\n",
    "    GROUP BY 1,2\n",
    "    )\n",
    "    WHERE Airport IN ('JFK', 'LGA', 'EWR')\n",
    "    GROUP BY 1,2\n",
    "    \"\"\"\n",
    "    \n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    with connection:\n",
    "        output = connection.execute(QUERY)\n",
    "    \n",
    "    day = []\n",
    "    airport = []\n",
    "    trip = []\n",
    "    \n",
    "    for row in output:\n",
    "        day.append(row[0])\n",
    "        airport.append(row[1])\n",
    "        trip.append(row[2])\n",
    "        \n",
    "    return day, airport, trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "day, airport, trip = get_data_for_visual_3()\n",
    "plot_visual_3(day, airport, trip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248147a9",
   "metadata": {},
   "source": [
    "## Visualization 4\n",
    "\n",
    "KeplerGL is used to create a geospatial visualization of the heatmap of all hired trips over a map of the bounded New York area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099580cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "map = KeplerGl(height=600, width=800)\n",
    "#show the map\n",
    "map.add_data(data=df4,name='New York City Taxi Trips')\n",
    "map.add_data(data=df5,name='New York City Uber Trips')\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838e651",
   "metadata": {},
   "source": [
    "## Visualization 5\n",
    "A scatter plot is created to compare tip amount vs distance for Yellow Taxi rides. We chose to use a scatter plot in order to best depict the relationship between the two variables. A line of best fit is also added. The general trend is that the is a positive relationship between tip amount and distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aecf8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_visual_5(data):\n",
    "    # preparing the dataset\n",
    "    tips = list(data.keys())\n",
    "    \n",
    "    distance = list(data.values())\n",
    "\n",
    "    fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "    #Creating the bar plot\n",
    "    plt.plot(tips, distance,'o', color='#f44336')\n",
    " \n",
    "    plt.xlabel(\"Tip Amount\", fontsize = 13)\n",
    "    plt.ylabel(\"Distance\", fontsize = 13)\n",
    "    plt.title(\"Graph of Tip Amount versus Distance for Yellow Taxi rides\", fontsize = 13)\n",
    "    \n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "    \n",
    "    x = np.array(tips)\n",
    "    y = np.array(distance)\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "\n",
    "    #Creating the line of best fit \n",
    "    plt.plot(x, a*x+b)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f1aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_5():\n",
    "    # Query SQL database for the data needed.\n",
    "    QUERY_V5 = \"\"\"\n",
    "    SELECT \n",
    "        tip_amount,\n",
    "        calculated_distance\n",
    "    FROM (taxi_trips\n",
    "    )\n",
    "    WHERE tip_amount IS NOT NULL AND calculated_distance IS NOT NULL AND tip_amount BETWEEN 0 and 40\n",
    "    \"\"\"\n",
    "    \n",
    "    with connection:\n",
    "        output = connection.execute(QUERY_V5)\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for row in output:\n",
    "        data[row[0]] = row[1]\n",
    " \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c0cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization5_dataframe = get_data_for_visual_5()\n",
    "plot_visual_5(visualization5_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6bb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c675a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4bdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77a29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e3e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909aab2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356702c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
