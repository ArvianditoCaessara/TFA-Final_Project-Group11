{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627e8d",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project._\n",
    "\n",
    "* Code clarity: make sure the code conforms to:\n",
    "    * [ ] [PEP 8](https://peps.python.org/pep-0008/) - You might find [this resource](https://realpython.com/python-pep8/) helpful as well as [this](https://github.com/dnanhkhoa/nb_black) or [this](https://jupyterlab-code-formatter.readthedocs.io/en/latest/) tool\n",
    "    * [ ] [PEP 257](https://peps.python.org/pep-0257/)\n",
    "    * [ ] Break each task down into logical functions\n",
    "* The following files are submitted for the project (see the project's GDoc for more details):\n",
    "    * [ ] `README.md`\n",
    "    * [ ] `requirements.txt`\n",
    "    * [ ] `.gitignore`\n",
    "    * [ ] `schema.sql`\n",
    "    * [ ] 6 query files (using the `.sql` extension), appropriately named for the purpose of the query\n",
    "    * [x] Jupyter Notebook containing the project (this file!)\n",
    "* [x] You can edit this cell and add a `x` inside the `[ ]` like this task to denote a completed task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import math\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import pyarrow.parquet as pq\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import datetime\n",
    "from math import sin, cos, sqrt, atan2, radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b622a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any general notebook setup, like log formatting\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need, for example:\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "# add other constants to refer to any local data, e.g. uber & weather\n",
    "UBER_CSV = \"uberdata/uber_rides_sample.csv\"\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9118da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder 'taxidata' because of .gitignore\n",
    "try:\n",
    "    os.mkdir(\"taxidata\")\n",
    "    os.mkdir(\"sql_files\")\n",
    "\n",
    "except FileExistsError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf38168",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Define a function that calculates the distance between two coordinates in kilometers that **only uses the `math` module** from the standard library.\n",
    "* [ ] Taxi data:\n",
    "    * [ ] Use the `re` module, and the packages `requests`, BeautifulSoup (`bs4`), and (optionally) `pandas` to programmatically download the required CSV files & load into memory.\n",
    "    * You may need to do this one file at a time - download, clean, sample. You can cache the sampling by saving it as a CSV file (and thereby freeing up memory on your computer) before moving onto the next file. \n",
    "* [ ] Weather & Uber data:\n",
    "    * [ ] Download the data manually in the link provided in the project doc.\n",
    "* [ ] All data:\n",
    "    * [ ] Load the data using `pandas`\n",
    "    * [ ] Clean the data, including:\n",
    "        * Remove unnecessary columns\n",
    "        * Remove invalid data points (take a moment to consider what's invalid)\n",
    "        * Normalize column names\n",
    "        * (Taxi & Uber data) Remove trips that start and/or end outside the designated [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "    * [ ] (Taxi data) Sample the data so that you have roughly the same amount of data points over the given date range for both Taxi data and Uber data.\n",
    "* [ ] Weather data:\n",
    "    * [ ] Split into two `pandas` DataFrames: one for required hourly data, and one for the required daily daya.\n",
    "    * [ ] You may find that the weather data you need later on does not exist at the frequency needed (daily vs hourly). You may calculate/generate samples from one to populate the other. Just document what you’re doing so we can follow along. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(start_lat, start_lon, end_lat, end_lon):\n",
    "    R = 6373.0\n",
    "\n",
    "    lat1 = radians(start_lat)\n",
    "    lon1 = radians(start_lon)\n",
    "    lat2 = radians(end_lat)\n",
    "    lon2 = radians(end_lon)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    \n",
    "    return round(distance, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(df):\n",
    "    distance = []\n",
    "    \n",
    "    for i in df.index:\n",
    "        estimated_distance = calculate_distance(df[\"pickup_latitude\"][i], df[\"pickup_longitude\"][i], df[\"dropoff_latitude\"][i], df[\"dropoff_longitude\"][i])\n",
    "        distance.append(estimated_distance)\n",
    "        \n",
    "    df[\"calculated_distance\"] = distance\n",
    "       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2132cbac",
   "metadata": {},
   "source": [
    "### Converting datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c637e",
   "metadata": {},
   "source": [
    "TODO: transform date columns from strings to datetime Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e46f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_str_to_obj(date_time_str):\n",
    "    date_time_obj = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return date_time_obj    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_csv_urls():\n",
    "    TAXI_URL = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "    \n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    data = []\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    for link in soup(\"a\"):\n",
    "        name = link.get(\"title\")\n",
    "        date = link.get(\"href\")\n",
    "        date_pattern = r'201[012345]|2009'\n",
    "        if name == \"Yellow Taxi Trip Records\" and re.search(date_pattern, date):\n",
    "            if not re.search(r'2015\\-0[789]|2015\\-1[012]', date):\n",
    "                data.append(link.get(\"href\"))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89ad2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_lon_from_loc():\n",
    "    find_lat_lon = gpd.read_file(\"taxi_zones/taxi_zones.shp\")\n",
    "  \n",
    "    find_lat_lon = find_lat_lon.to_crs(4326)\n",
    "    lon = find_lat_lon.centroid.x \n",
    "    lat = find_lat_lon.centroid.y\n",
    "    find_lat_lon[\"lon\"] = lon\n",
    "    find_lat_lon[\"lat\"] = lat\n",
    "\n",
    "    return find_lat_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "459739ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lat_lon(df):\n",
    "    lon_border = [NEW_YORK_BOX_COORDS[0][1], NEW_YORK_BOX_COORDS[1][1]]\n",
    "    lat_border = [NEW_YORK_BOX_COORDS[0][0], NEW_YORK_BOX_COORDS[1][0]]\n",
    "    \n",
    "    deleted_row = []\n",
    "    \n",
    "    for i in df.index:\n",
    "        if df[\"pickup_longitude\"][i] < lon_border[0] or df[\"pickup_longitude\"][i] > lon_border[1] or df[\"dropoff_longitude\"][i] < lon_border[0] or df[\"dropoff_longitude\"][i] > lon_border[1]:\n",
    "            deleted_row.append(i)\n",
    "            \n",
    "        elif df[\"pickup_latitude\"][i] < lat_border[0] or df[\"pickup_latitude\"][i] > lat_border[1] or df[\"dropoff_latitude\"][i] < lat_border[0] or df[\"dropoff_latitude\"][i] > lat_border[1]:\n",
    "            deleted_row.append(i)\n",
    "            \n",
    "            \n",
    "    df = df.drop(labels = deleted_row, axis=0)\n",
    "    \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdcad8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # Normalize column for 2009 data\n",
    "    if \"Passenger_Count\" in df.columns :\n",
    "        df.rename(columns={\"Passenger_Count\": 'passenger_count', \"Fare_Amt\": 'fare_amount', \"Trip_Distance\": 'trip_distance'}, inplace=True)\n",
    "    \n",
    "    deleted_row = []\n",
    "    \n",
    "    for i in df.index:        \n",
    "        # Trips with zero passenger count\n",
    "        if df[\"passenger_count\"][i] < 1 or df[\"passenger_count\"][i] == False:\n",
    "            deleted_row.append(i)\n",
    "            \n",
    "        # Trips with no fare\n",
    "        elif df[\"fare_amount\"][i] <= 0 or df[\"fare_amount\"][i] == False:\n",
    "            deleted_row.append(i)\n",
    "            \n",
    "        # Trips with no distance between dropoff and pickup\n",
    "        elif df[\"trip_distance\"][i] <= 0 or df[\"trip_distance\"][i] == False:\n",
    "            deleted_row.append(i)\n",
    "            \n",
    "    df = df.drop(labels = deleted_row, axis=0)\n",
    "    \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url):\n",
    "    \n",
    "    # Check if we already have the data in local, otherwise download it\n",
    "    file_name = url[-31:]\n",
    "    \n",
    "    for i in os.listdir('./taxidata'):\n",
    "        if i == file_name:\n",
    "            df = pd.read_parquet(f\"taxidata/{file_name}\", engine='pyarrow')\n",
    "            return df\n",
    "    \n",
    "    \n",
    "    # Download and preprocessing data\n",
    "    find_lat_lon = get_lat_lon_from_loc() #get the lat-lon form .shp file\n",
    "    \n",
    "    # Read the data from url\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(f\"taxidata/{url[-31:]}\", \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    df = pd.read_parquet(f\"taxidata/{url[-31:]}\", engine='pyarrow') #Reading the data\n",
    "    df = df.sample(n=2564).reset_index(drop=True) #Take 2564 random sample\n",
    "\n",
    "    try:\n",
    "        #Clean the data\n",
    "        df = clean_data(df)\n",
    "    \n",
    "        # Rename the columns\n",
    "        if 'Start_Lon' in df.columns : #2009\n",
    "            df = df[[\"Trip_Pickup_DateTime\", \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\"]]\n",
    "            df.rename(columns={\"Trip_Pickup_DateTime\": 'date_time', \"Start_Lon\": 'pickup_longitude', \"Start_Lat\": 'pickup_latitude', \"End_Lon\": 'dropoff_longitude', \"End_Lat\": 'dropoff_latitude'}, inplace=True)\n",
    "        \n",
    "        elif 'pickup_longitude' in df.columns : #2010\n",
    "            df = df[[\"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]]\n",
    "            df.rename(columns={\"pickup_datetime\": 'date_time'}, inplace=True)\n",
    "\n",
    "        else:     \n",
    "            if \"tpep_pickup_datetime\" in df.columns : #2011-2014\n",
    "                df = df[[\"tpep_pickup_datetime\", \"PULocationID\", \"DOLocationID\"]]\n",
    "                df.rename(columns={\"tpep_pickup_datetime\": 'date_time'}, inplace=True)\n",
    "        \n",
    "            else: df = df[[\"date_time\", \"trip_distance\", \"PULocationID\", \"DOLocationID\"]] #2015\n",
    "            \n",
    "            \n",
    "            # Finding the lat-lon using shp file\n",
    "            start_lon = []\n",
    "            start_lat = []\n",
    "            end_lon = []\n",
    "            end_lat = []\n",
    "        \n",
    "        \n",
    "            for i in range(len(df[\"PULocationID\"])):\n",
    "                start_point = df[\"PULocationID\"][i]\n",
    "                end_point = df[\"DOLocationID\"][i]\n",
    "            \n",
    "                if df[\"PULocationID\"][i] < 264 and df[\"DOLocationID\"][i] < 264: #Filter for NYC Area only\n",
    "                    index_location = find_lat_lon[find_lat_lon[\"LocationID\"] == start_point].index.values[0] \n",
    "                    start_lon.append(float(find_lat_lon[\"lon\"][index_location]))\n",
    "                    start_lat.append(float(find_lat_lon[\"lat\"][index_location]))\n",
    "                \n",
    "                    index_location = find_lat_lon[find_lat_lon[\"LocationID\"] == end_point].index.values[0] \n",
    "                    end_lon.append(float(find_lat_lon[\"lon\"][index_location]))\n",
    "                    end_lat.append(float(find_lat_lon[\"lat\"][index_location]))\n",
    "                \n",
    "                else: # Area outside NYC, to be deleted later\n",
    "                    start_lon.append(0)\n",
    "                    start_lat.append(0)\n",
    "                    end_lon.append(0)\n",
    "                    end_lat.append(0)\n",
    "                    \n",
    "        \n",
    "                     \n",
    "            df[\"pickup_longitude\"] = start_lon\n",
    "            df[\"pickup_latitude\"] = start_lat\n",
    "            df[\"dropoff_longitude\"] = end_lon\n",
    "            df[\"dropoff_latitude\"] = end_lat\n",
    "        \n",
    "            df = df.drop([\"PULocationID\", \"DOLocationID\"], axis=1)\n",
    "        \n",
    "       \n",
    "        \n",
    "        # Filter the lat-lon between (40.560445, -74.242330) and (40.908524, -73.717047)\n",
    "        df = filter_lat_lon(df)\n",
    "    \n",
    "        #Convert datetime str to python object\n",
    "        if isinstance(df[\"date_time\"][0], str):\n",
    "            print(\"str nee\")\n",
    "            for i in df.index:\n",
    "                df[\"date_time\"][i] = datetime_str_to_obj(df[\"date_time\"][i])\n",
    "    \n",
    "        # Calculate distance and add calculated_distance column\n",
    "        df = add_distance_column(df)\n",
    "        \n",
    "    except IndexError:\n",
    "        os.remove(f\"taxidata/{url[-31:]}\")\n",
    "        get_and_clean_month_taxi_data(url)\n",
    "    \n",
    "    df = df[[\"date_time\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"calculated_distance\"]]\n",
    "    \n",
    "    # Re-save the file\n",
    "    df.to_parquet(f\"taxidata/{url[-31:]}\")\n",
    "    \n",
    "    return df            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4c49a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBD\n",
    "#url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet'\n",
    "#x = get_and_clean_month_taxi_data(url)\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_csv_urls = find_taxi_csv_urls()\n",
    "    for csv_url in all_csv_urls:\n",
    "        # Get and clean the data from local or url\n",
    "        dataframe = get_and_clean_month_taxi_data(csv_url)\n",
    "        \n",
    "        # Put all the dataframe into a list\n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # Create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes, ignore_index=True)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f8425bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_uber_data(df):\n",
    "    \n",
    "    deleted_row = []\n",
    "    \n",
    "    for i in df.index:\n",
    "        # Convert datetime str to python object\n",
    "        df['pickup_datetime'][i] = df['pickup_datetime'][i].replace(' UTC', '')\n",
    "        df['pickup_datetime'][i] = df['pickup_datetime'][i].replace('T', ' ')\n",
    "        df['pickup_datetime'][i] = datetime_str_to_obj(df['pickup_datetime'][i])\n",
    "\n",
    "        # Trips with zero passenger count\n",
    "        if df[\"passenger_count\"][i] < 1 or df[\"passenger_count\"][i] == False:\n",
    "            deleted_row.append(i)\n",
    "            \n",
    "        # Trips with no fare\n",
    "        elif df[\"fare_amount\"][i] <= 0 or df[\"fare_amount\"][i] == False:\n",
    "            deleted_row.append(i)\n",
    "            \n",
    "    df = df.drop(labels = deleted_row, axis=0)\n",
    "    df.rename(columns={\"pickup_datetime\": 'date_time'}, inplace=True)\n",
    "\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = clean_uber_data(df)\n",
    "    df = filter_lat_lon(df)\n",
    "    # Drop unecesarry column\n",
    "    df.drop(df.columns[[0, 1, 2, 8]], axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    df2 = pd.read_csv('weatherdata/' + csv_file)\n",
    "    df2 = df2[['DATE', 'LATITUDE', 'LONGITUDE', 'HourlyPrecipitation', 'HourlyWindSpeed']]\n",
    "    df2 = df2.dropna()    \n",
    "    \n",
    "    deleted_row = []\n",
    "    \n",
    "    for i in df2.index:\n",
    "        # Convert datetime str to python object\n",
    "        df2['DATE'][i] = df2['DATE'][i].replace('T', ' ')\n",
    "        df2['DATE'][i] = datetime_str_to_obj(df2['DATE'][i])\n",
    "    \n",
    "        try:\n",
    "            df2[\"HourlyPrecipitation\"][i] = float(df2[\"HourlyPrecipitation\"][i])\n",
    "            df2[\"HourlyWindSpeed\"][i] = int(df2[\"HourlyWindSpeed\"][i])\n",
    "            \n",
    "        \n",
    "            if(df2[\"HourlyPrecipitation\"][i] <= 0 or df2[\"HourlyPrecipitation\"][i] == False):\n",
    "                deleted_row.append(i)\n",
    "            elif (df2[\"HourlyWindSpeed\"][i] <= 0 or df2[\"HourlyWindSpeed\"][i] == False):\n",
    "                deleted_row.append(i)\n",
    "            \n",
    "        except ValueError:\n",
    "            deleted_row.append(i)\n",
    "       \n",
    "            \n",
    "    df2 = df2.drop(labels = deleted_row, axis=0)\n",
    "    \n",
    "    return df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    df3 = pd.read_csv('weatherdata/' + csv_file)\n",
    "    df3 = df3[['DATE', 'LATITUDE', 'LONGITUDE', 'DailyAverageWindSpeed', 'DailyPrecipitation', 'Sunrise', 'Sunset']]\n",
    "    df3.dropna()\n",
    "    \n",
    "    deleted_row = []\n",
    "    \n",
    "    for i in df3.index:\n",
    "        # Convert datetime str to python object\n",
    "        df3['DATE'][i] = df3['DATE'][i].replace('T', ' ')\n",
    "        df3['DATE'][i] = datetime_str_to_obj(df3['DATE'][i])\n",
    "    \n",
    "        try:\n",
    "            df3[\"DailyPrecipitation\"][i] = float(df3[\"DailyPrecipitation\"][i])\n",
    "            df3[\"DailyAverageWindSpeed\"][i] = int(df3[\"DailyAverageWindSpeed\"][i])\n",
    "            df3[\"Sunrise\"][i] = int(df3[\"Sunrise\"][i])\n",
    "            df3[\"Sunset\"][i] = int(df3[\"Sunset\"][i])\n",
    "        \n",
    "            if(df3[\"DailyPrecipitation\"][i] <= 0 or df3[\"DailyPrecipitation\"][i] == False):\n",
    "                deleted_row.append(i)\n",
    "            elif (df3[\"DailyAverageWindSpeed\"][i] <= 0 or df3[\"DailyAverageWindSpeed\"][i] == False):\n",
    "                deleted_row.append(i)\n",
    "            \n",
    "        except ValueError:\n",
    "            deleted_row.append(i)\n",
    "       \n",
    "            \n",
    "    df3 = df3.drop(labels = deleted_row, axis=0)\n",
    "    \n",
    "    return df3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    # add some way to find all weather CSV files or just add the name/paths manually\n",
    "    weather_csv_files = [\"2009_weather.csv\", \"2010_weather.csv\", \"2011_weather.csv\", \"2012_weather.csv\", \"2013_weather.csv\", \"2014_weather.csv\", \"2015_weather.csv\"]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes, ignore_index=True)\n",
    "    daily_data = pd.concat(daily_dataframes, ignore_index=True)\n",
    "    daily_sunrisesunset_data = daily_data[[\"Sunrise\", \"Sunset\"]]\n",
    "    \n",
    "    daily_data = daily_data.drop([\"Sunrise\", \"Sunset\"], axis = 1)\n",
    "    \n",
    "    \n",
    "    return hourly_data, daily_data, daily_sunrisesunset_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "_This is where you can actually execute all the required functions._\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_and_clean_taxi_data()\n",
    "uber_data = get_uber_data()\n",
    "hourly_weather_data, daily_weather_data, daily_sunrisesunset_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a83ceb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>calculated_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-19 11:35:42</td>\n",
       "      <td>-73.981532</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.965554</td>\n",
       "      <td>40.782478</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-24 20:04:46</td>\n",
       "      <td>-74.001538</td>\n",
       "      <td>40.723888</td>\n",
       "      <td>-73.985937</td>\n",
       "      <td>40.727620</td>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-06 12:38:50</td>\n",
       "      <td>-73.984052</td>\n",
       "      <td>40.736824</td>\n",
       "      <td>-73.984052</td>\n",
       "      <td>40.736824</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-07 01:09:14</td>\n",
       "      <td>-73.984052</td>\n",
       "      <td>40.736824</td>\n",
       "      <td>-73.959635</td>\n",
       "      <td>40.766948</td>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-07 23:16:13</td>\n",
       "      <td>-73.995135</td>\n",
       "      <td>40.766238</td>\n",
       "      <td>-73.996971</td>\n",
       "      <td>40.742279</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193921</th>\n",
       "      <td>2009-12-28 17:56:00</td>\n",
       "      <td>-73.992052</td>\n",
       "      <td>40.749815</td>\n",
       "      <td>-74.004377</td>\n",
       "      <td>40.719260</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193922</th>\n",
       "      <td>2009-12-05 16:58:42</td>\n",
       "      <td>-73.965277</td>\n",
       "      <td>40.760410</td>\n",
       "      <td>-73.997142</td>\n",
       "      <td>40.736587</td>\n",
       "      <td>3.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193923</th>\n",
       "      <td>2009-12-15 23:28:00</td>\n",
       "      <td>-73.873485</td>\n",
       "      <td>40.773988</td>\n",
       "      <td>-73.950458</td>\n",
       "      <td>40.783840</td>\n",
       "      <td>6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193924</th>\n",
       "      <td>2009-12-05 14:44:11</td>\n",
       "      <td>-73.960692</td>\n",
       "      <td>40.801664</td>\n",
       "      <td>-73.956926</td>\n",
       "      <td>40.785103</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193925</th>\n",
       "      <td>2009-12-20 09:32:00</td>\n",
       "      <td>-73.982237</td>\n",
       "      <td>40.771030</td>\n",
       "      <td>-73.978102</td>\n",
       "      <td>40.752975</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193926 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date_time  pickup_longitude  pickup_latitude  \\\n",
       "0      2015-01-19 11:35:42        -73.981532        40.773633   \n",
       "1      2015-01-24 20:04:46        -74.001538        40.723888   \n",
       "2      2015-01-06 12:38:50        -73.984052        40.736824   \n",
       "3      2015-01-07 01:09:14        -73.984052        40.736824   \n",
       "4      2015-01-07 23:16:13        -73.995135        40.766238   \n",
       "...                    ...               ...              ...   \n",
       "193921 2009-12-28 17:56:00        -73.992052        40.749815   \n",
       "193922 2009-12-05 16:58:42        -73.965277        40.760410   \n",
       "193923 2009-12-15 23:28:00        -73.873485        40.773988   \n",
       "193924 2009-12-05 14:44:11        -73.960692        40.801664   \n",
       "193925 2009-12-20 09:32:00        -73.982237        40.771030   \n",
       "\n",
       "        dropoff_longitude  dropoff_latitude  calculated_distance  \n",
       "0              -73.965554         40.782478                 1.67  \n",
       "1              -73.985937         40.727620                 1.38  \n",
       "2              -73.984052         40.736824                 0.00  \n",
       "3              -73.959635         40.766948                 3.93  \n",
       "4              -73.996971         40.742279                 2.67  \n",
       "...                   ...               ...                  ...  \n",
       "193921         -74.004377         40.719260                 3.55  \n",
       "193922         -73.997142         40.736587                 3.77  \n",
       "193923         -73.950458         40.783840                 6.58  \n",
       "193924         -73.956926         40.785103                 1.87  \n",
       "193925         -73.978102         40.752975                 2.04  \n",
       "\n",
       "[193926 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter taxi data\n",
    "taxi_data = taxi_data[[\"date_time\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"calculated_distance\"]]\n",
    "taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eab82f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sunrise</th>\n",
       "      <th>Sunset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04:53:00</td>\n",
       "      <td>19:11:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04:57:00</td>\n",
       "      <td>19:06:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04:58:00</td>\n",
       "      <td>19:05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05:02:00</td>\n",
       "      <td>19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05:06:00</td>\n",
       "      <td>18:54:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>07:19:00</td>\n",
       "      <td>16:35:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>07:19:00</td>\n",
       "      <td>16:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>07:20:00</td>\n",
       "      <td>16:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>07:20:00</td>\n",
       "      <td>16:37:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>07:20:00</td>\n",
       "      <td>16:38:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>405 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sunrise    Sunset\n",
       "0    04:53:00  19:11:00\n",
       "1    04:57:00  19:06:00\n",
       "2    04:58:00  19:05:00\n",
       "3    05:02:00  19:00:00\n",
       "4    05:06:00  18:54:00\n",
       "..        ...       ...\n",
       "400  07:19:00  16:35:00\n",
       "401  07:19:00  16:36:00\n",
       "402  07:20:00  16:36:00\n",
       "403  07:20:00  16:37:00\n",
       "404  07:20:00  16:38:00\n",
       "\n",
       "[405 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Update the timesetting of sunrise-sunset\n",
    "\n",
    "for i in daily_sunrisesunset_data.index:\n",
    "    x = str(daily_sunrisesunset_data[\"Sunrise\"][i])\n",
    "    daily_sunrisesunset_data[\"Sunrise\"][i] = f\"{x[:-4]}:{x[-4:-2]}\"\n",
    "    daily_sunrisesunset_data[\"Sunrise\"][i] = datetime.datetime.strptime(daily_sunrisesunset_data[\"Sunrise\"][i], '%H:%M').time()\n",
    "    y = str(daily_sunrisesunset_data[\"Sunset\"][i])\n",
    "    daily_sunrisesunset_data[\"Sunset\"][i] = f\"{y[:-4]}:{y[-4:-2]}\"\n",
    "    daily_sunrisesunset_data[\"Sunset\"][i] = datetime.datetime.strptime(daily_sunrisesunset_data[\"Sunset\"][i], '%H:%M').time()\n",
    "\n",
    "daily_sunrisesunset_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "_Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Connection at 0x7fd48b84d8a0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "connection = sqlite3.connect(\"final_project.db\")\n",
    "connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATETIME,\n",
    "    LATITUDE FLOAT,\n",
    "    LONGITUDE FLOAT,\n",
    "    HourlyPrecipitation FLOAT,\n",
    "    HourlyWindSpeed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATETIME,\n",
    "    LATITUDE FLOAT,\n",
    "    LONGITUDE FLOAT,\n",
    "    DailyPrecipitation FLOAT,\n",
    "    DailyAverageWindSpeed FLOAT,\n",
    "    Sunrise FLOAT,\n",
    "    Sunset FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date_time DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    calculated_distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date_time DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    calculated_distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_SUNRISE_SUNSET_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_sunrise_sunset\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    Sunrise TIMESTAMP,\n",
    "    Sunset TIMESTAMP,\n",
    "    FOREIGN KEY(id) REFERENCES daily_weather(id)\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open('sql_files/schema.sql', \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)\n",
    "    f.write(DAILY_SUNRISE_SUNSET_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc82fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with connection:\n",
    "    connection.execute(HOURLY_WEATHER_SCHEMA)\n",
    "with connection:\n",
    "    connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "with connection:\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "with connection:\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)\n",
    "with connection:\n",
    "    connection.execute(DAILY_SUNRISE_SUNSET_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "137365e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for key, value in table_to_df_dict.items():\n",
    "        print(key)\n",
    "        value.to_sql(name=key, con=connection, if_exists='append', index=False)\n",
    "    \n",
    "    return 'Success adding data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "    \"daily_sunrise_sunset\": daily_sunrisesunset_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bb6e42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxi_trips\n",
      "uber_trips\n",
      "hourly_weather\n",
      "daily_weather\n",
      "daily_sunrise_sunset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Success adding data'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4753fcd",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins.\n",
    "* [ ] For the same time frame, what day of the week was the most popular to take an uber? The result should have 7 bins.\n",
    "* [ ] What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "* [ ] What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?\n",
    "* [ ] Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?\n",
    "* [ ] During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(query)\n",
    "    \n",
    "    return f'Succes generate {outfile}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77020452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1) For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? \n",
    "#The result should have 24 bins.\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT \n",
    "    DISTINCT strftime('%H', date_time) AS time,\n",
    "    COUNT (*) as trip\n",
    "FROM taxi_trips\n",
    "WHERE date_time between '2009-01-01' AND '2015-06-30'\n",
    "GROUP BY time\n",
    "ORDER BY trip DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('19', 24024)\n",
      "('18', 23312)\n",
      "('20', 22600)\n",
      "('21', 22144)\n",
      "('22', 21706)\n",
      "('23', 19372)\n",
      "('14', 19316)\n",
      "('12', 19234)\n",
      "('17', 19198)\n",
      "('15', 18856)\n",
      "('13', 18668)\n",
      "('09', 18484)\n",
      "('11', 18474)\n",
      "('08', 17464)\n",
      "('10', 17308)\n",
      "('16', 15860)\n",
      "('00', 15350)\n",
      "('07', 13964)\n",
      "('01', 11458)\n",
      "('02', 8402)\n",
      "('06', 7888)\n",
      "('03', 6368)\n",
      "('04', 4524)\n",
      "('05', 3708)\n"
     ]
    }
   ],
   "source": [
    "# TOBEDELETED Read Data\n",
    "with connection:\n",
    "    result = connection.execute(QUERY_1)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ed9cace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Succes generate sql_files/question_1.sql'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_query_to_file(QUERY_1, \"sql_files/question_1.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c8902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2) For the same time frame, what day of the week was the most popular to take an uber? \n",
    "#The result should have 7 bins.\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT  case cast (strftime('%w', date_time) as integer)\n",
    "  when 0 then 'Sunday'\n",
    "  when 1 then 'Monday'\n",
    "  when 2 then 'Tuesday'\n",
    "  when 3 then 'Wednesday'\n",
    "  when 4 then 'Thursday'\n",
    "  when 5 then 'Friday'\n",
    "  else 'Saturday' end as day,\n",
    "  COUNT(*) as no_of_trip\n",
    "FROM uber_trips\n",
    "WHERE date_time between '2009-01-01' AND '2015-06-30'\n",
    "GROUP BY day\n",
    "ORDER BY no_of_trip DESC\n",
    "\"\"\"\n",
    "\n",
    "# WHERE tpep_pickup_datetime between '2009-01-01' AND '2015-06-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85904b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOBEDELETED Read Data\n",
    "with connection:\n",
    "    result = connection.execute(QUERY_2)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d989dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, \"sql_files/question_2.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3) What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "# TBD! FILTER DATE BLM, MASIH BINGUNG MAU YG DI OERCENTILE\n",
    "\n",
    "QUERY_3 = \"\"\"\n",
    "WITH \n",
    "base \n",
    "AS (\n",
    "    SELECT\n",
    "        date,\n",
    "        calculated_distance,\n",
    "        ROW_NUMBER() OVER(ORDER BY calculated_distance ASC) AS row_num\n",
    "    FROM (\n",
    "        SELECT date(date_time) as date, calculated_distance\n",
    "        FROM taxi_trips\n",
    "        WHERE date between '2013-07-01' AND '2013-07-31'\n",
    "        UNION ALL\n",
    "        SELECT date(date_time) as date, calculated_distance\n",
    "        FROM uber_trips\n",
    "        WHERE date between '2013-07-01' AND '2013-07-31'\n",
    "    )\n",
    "    WHERE date between '2013-07-01' AND '2013-07-31'\n",
    "    ),\n",
    "    \n",
    "quantile\n",
    "AS (\n",
    "    SELECT\n",
    "        round(0.95 * COUNT(calculated_distance)) AS n_quantile\n",
    "    FROM\n",
    "        base\n",
    "    )\n",
    "    \n",
    "select \n",
    "base.calculated_distance \n",
    "from base\n",
    "join quantile\n",
    "on base.row_num = quantile.n_quantile\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219138f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOBEDELETED Read Data # WHERE percent_rank >= 0.95\n",
    "with connection:\n",
    "    result = connection.execute(QUERY_3)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7c43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, \"sql_files/question_3.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4) What were the top 10 days with the highest number of hired rides for 2009, \n",
    "# and what was the average distance for each day?\n",
    "\n",
    "QUERY_4 = \"\"\"\n",
    "SELECT\n",
    "    date,\n",
    "    COUNT (*) as no_of_trip,\n",
    "    AVG (calculated_distance)\n",
    "FROM (\n",
    "    SELECT date(date_time) AS date, calculated_distance\n",
    "    FROM taxi_trips\n",
    "    UNION ALL\n",
    "    SELECT date(date_time) AS date, calculated_distance\n",
    "    FROM uber_trips\n",
    ")\n",
    "WHERE date between '2009-01-01' AND '2009-12-31'\n",
    "GROUP BY date\n",
    "ORDER BY no_of_trip DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81daf27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOBEDELETED Read Data\n",
    "with connection:\n",
    "    result = connection.execute(QUERY_4)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b57a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, \"sql_files/question_4.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5) Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?\n",
    "\n",
    "QUERY_5 = \"\"\"\n",
    "WITH\n",
    "weather AS (\n",
    "SELECT\n",
    "    date(DATE) as date,\n",
    "    DailyAverageWindSpeed\n",
    "FROM\n",
    "    daily_weather\n",
    "WHERE date between '2014-01-01' AND '2014-12-31'\n",
    "ORDER BY DailyAverageWindSpeed DESC\n",
    "LIMIT 10),\n",
    "\n",
    "trip AS (\n",
    "SELECT\n",
    "    date,\n",
    "    COUNT (*) as no_of_trip\n",
    "FROM (\n",
    "    SELECT date(date_time) AS date\n",
    "    FROM taxi_trips\n",
    "    WHERE date between '2014-01-01' AND '2014-12-31'\n",
    "    UNION ALL\n",
    "    SELECT date(date_time) AS date\n",
    "    FROM uber_trips\n",
    "    WHERE date between '2014-01-01' AND '2014-12-31'\n",
    ")\n",
    "GROUP BY date\n",
    ")\n",
    "\n",
    "\n",
    "SELECT\n",
    "    weather.*,\n",
    "    trip.no_of_trip\n",
    "FROM\n",
    "    weather\n",
    "LEFT JOIN trip\n",
    "ON weather.date = trip.date\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13876397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOBEDELETED Read Data\n",
    "with connection:\n",
    "    result = connection.execute(QUERY_5)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, \"sql_files/question_5.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628bacbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6) During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, \n",
    "# how many trips were taken each hour, and for each hour, \n",
    "# how much precipitation did NYC receive and what was the sustained wind speed?\n",
    "#starting 22nd Oct\n",
    "\n",
    "QUERY_6 = \"\"\"\n",
    "WITH\n",
    "weather AS (\n",
    "SELECT\n",
    "    strftime('%Y-%m-%d', DATE) AS date,\n",
    "    strftime('%H', DATE) AS time,\n",
    "    sum(HourlyPrecipitation) as Precipitation,\n",
    "    avg(HourlyWindSpeed) as Wind_Speed\n",
    "FROM\n",
    "    hourly_weather\n",
    "WHERE date between '2012-10-22' AND '2012-10-31'\n",
    "GROUP BY time\n",
    "),\n",
    "\n",
    "trip AS (\n",
    "SELECT\n",
    "    date,\n",
    "    time,\n",
    "    COUNT (*) as no_of_trip\n",
    "FROM(\n",
    "    SELECT \n",
    "        date(date_time) AS date, \n",
    "        strftime('%H', date_time) AS time\n",
    "    FROM taxi_trips\n",
    "    WHERE date between '2012-10-22' AND '2012-10-31'\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        date(date_time) AS date, \n",
    "        strftime('%H', date_time) AS time\n",
    "    FROM uber_trips\n",
    "    WHERE date between '2012-10-22' AND '2012-10-31'\n",
    ")\n",
    "GROUP BY date, time\n",
    ")\n",
    "\n",
    "\n",
    "SELECT\n",
    "    weather.date,\n",
    "    weather.time,\n",
    "    trip.no_of_trip,\n",
    "    weather.Precipitation,\n",
    "    weather.Wind_Speed\n",
    "FROM\n",
    "    weather\n",
    "LEFT JOIN trip\n",
    "ON weather.time = trip.time AND weather.date = trip.date\n",
    "ORDER BY weather.date\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOBEDELETED Read Data\n",
    "with connection:\n",
    "    result = connection.execute(QUERY_6)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, \"sql_files/question_6.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29e6daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra question regarding the sunrise-sunset \n",
    "# What are the top 10 sunrise times with highest average wind speed?\n",
    "\n",
    "QUERY_EXTRA = \"\"\"\n",
    "SELECT\n",
    "    strftime('%H:%M', x.Sunrise) AS time,\n",
    "    avg(y.DailyAverageWindSpeed) AS average_windspeed\n",
    "FROM\n",
    "    daily_sunrise_sunset AS x\n",
    "JOIN daily_weather AS Y\n",
    "ON x.id = Y.id\n",
    "GROUP BY time\n",
    "ORDER BY average_windspeed DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2f66984",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('06:59', 13.0)\n",
      "('05:53', 12.0)\n",
      "('06:24', 11.5)\n",
      "('06:34', 10.0)\n",
      "('05:45', 10.0)\n",
      "('06:50', 9.0)\n",
      "('06:37', 9.0)\n",
      "('05:35', 9.0)\n",
      "('07:15', 8.5)\n",
      "('06:33', 8.5)\n"
     ]
    }
   ],
   "source": [
    "#TOBEDELETED Read Data\n",
    "with connection:\n",
    "    result = connection.execute(QUERY_EXTRA)\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c990ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Succes generate sql_files/question_extra.sql'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_query_to_file(QUERY_EXTRA, \"sql_files/question_extra.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Create an appropriate visualization for the first query/question in part 3\n",
    "* [ ] Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization\n",
    "* [ ] Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport.\n",
    "* [ ] Create a heatmap of all hired trips over a map of the area. Consider using KeplerGL or another library that helps generate geospatial visualizations.\n",
    "* [ ] Create a scatter plot that compares tip amount versus distance.\n",
    "* [ ] Create another scatter plot that compares tip amount versus precipitation amount.\n",
    "\n",
    "_Be sure these cells are executed so that the visualizations are rendered when the notebook is submitted._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each visualization._\n",
    "\n",
    "_The example below makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_n(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_n():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some_dataframe = get_data_for_visual_n()\n",
    "#plot_visual_n(some_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1611d2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
